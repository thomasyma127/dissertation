{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "064e0f40-1e84-49c2-8a5c-029bb2634a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from scipy.special import erf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1270abb5-8a8b-4ed2-a4fc-6ae343aae58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIMATE_DIR = \"climate\"\n",
    "CURR_KEY = \"20201201-20301130\"\n",
    "FUT_KEY  = \"20701201-20801130\"\n",
    "\n",
    "\n",
    "RAIL_SHP = \"railshape/NWR_TrackCentreLines.shp\"\n",
    "OLE_SHP  = \"oleshape/oleshape.shp\"\n",
    "\n",
    "\n",
    "OUT_DIR = \"outputs_final_with_shapes5\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "OPEN_WITH_DASK = True\n",
    "TIME_CHUNK = 360\n",
    "\n",
    "THR = 0.05\n",
    "\n",
    "\n",
    "RAIL_MU0, RAIL_SIG = 34.0, 2.5\n",
    "OLE_MU0,  OLE_SIG  = 33.0, 2.0\n",
    "\n",
    "\n",
    "RAIL_MU1, OLE_MU1 = 41.5, 53.0\n",
    "\n",
    "\n",
    "RAIL_TON_PER_KM = 1.96\n",
    "OLE_TON_PER_KM  = 0.957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f3510a-7b38-47fa-b616-452d28d4e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_period_nc(climate_dir: str, keyword: str) -> xr.Dataset:\n",
    "\n",
    "    files = sorted([f for f in glob(os.path.join(climate_dir, \"*.nc\")) if keyword in os.path.basename(f)])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"not found {keyword}  .nc files\")\n",
    "\n",
    "    ds_list = []\n",
    "    for fp in files:\n",
    "        ds = xr.open_dataset(fp, decode_times=False)\n",
    "\n",
    "        var = None\n",
    "        for name in ['tasmax','tas','TXx','t2m','temperature','air_temperature']:\n",
    "            if name in ds.data_vars and 'time' in ds[name].dims and ds[name].ndim >= 3:\n",
    "                var = name; break\n",
    "        if var is None:\n",
    "            for name, v in ds.data_vars.items():\n",
    "                if v.ndim >= 3 and 'time' in v.dims:\n",
    "                    var = name; break\n",
    "        if var is None:\n",
    "            raise ValueError(f\"{os.path.basename(fp)} notfound2\")\n",
    "\n",
    "        v = ds[var]\n",
    "\n",
    "        rename_map = {}\n",
    "        for d in v.dims:\n",
    "            dl = d.lower()\n",
    "            if dl.startswith('ensemble'):\n",
    "                rename_map[d] = 'member'\n",
    "            elif 'projection_y' in dl or d == 'y':\n",
    "                rename_map[d] = 'y'\n",
    "            elif 'projection_x' in dl or d == 'x':\n",
    "                rename_map[d] = 'x'\n",
    "            elif d == 'time':\n",
    "                pass\n",
    "        v = v.rename(rename_map)\n",
    "        if 'member' not in v.dims:\n",
    "            v = v.expand_dims('member')\n",
    "\n",
    "        dsn = xr.Dataset()\n",
    "        dsn['temp'] = v.astype('float32')\n",
    "        units = v.attrs.get('units','').lower()\n",
    "        if units in ['k','kelvin']:\n",
    "            dsn['temp'] = dsn['temp'] - 273.15\n",
    "        dsn['temp'].attrs['units'] = 'degC'\n",
    "\n",
    "\n",
    "        if 'projection_x_coordinate' in ds.coords:\n",
    "            dsn = dsn.assign_coords(x=ds['projection_x_coordinate'].rename({'projection_x_coordinate':'x'}))\n",
    "        if 'projection_y_coordinate' in ds.coords:\n",
    "            dsn = dsn.assign_coords(y=ds['projection_y_coordinate'].rename({'projection_y_coordinate':'y'}))\n",
    "        if 'projection_x_coordinate_bnds' in ds.variables:\n",
    "            dsn['x_bnds'] = ds['projection_x_coordinate_bnds'].rename({'projection_x_coordinate':'x'})\n",
    "        if 'projection_y_coordinate_bnds' in ds.variables:\n",
    "            dsn['y_bnds'] = ds['projection_y_coordinate_bnds'].rename({'projection_y_coordinate':'y'})\n",
    "\n",
    "\n",
    "        if 'grid_latitude' in ds.variables:\n",
    "            lat = ds['grid_latitude']\n",
    "            if lat.ndim == 2:\n",
    "                lat = lat.rename({lat.dims[-2]: 'y', lat.dims[-1]: 'x'})\n",
    "            dsn = dsn.assign_coords(lat=lat)\n",
    "\n",
    "        if 'grid_longitude' in ds.variables:\n",
    "            lon = ds['grid_longitude']\n",
    "            if lon.ndim == 2:\n",
    "                lon = lon.rename({lon.dims[-2]: 'y', lon.dims[-1]: 'x'}) \n",
    "            dsn = dsn.assign_coords(lon=lon)\n",
    "\n",
    "\n",
    "\n",
    "        for tname in ['yyyymmdd','year','month_number']:\n",
    "            if tname in ds.variables:\n",
    "                dsn[tname] = ds[tname]\n",
    "\n",
    "        # grid mapping\n",
    "        if 'transverse_mercator' in ds.variables:\n",
    "            dsn['transverse_mercator'] = ds['transverse_mercator']\n",
    "\n",
    "        ds_list.append(dsn)\n",
    "\n",
    "    ds_all = xr.concat(ds_list, dim='time')\n",
    "    if OPEN_WITH_DASK:\n",
    "        ds_all = ds_all.chunk({'time': TIME_CHUNK})\n",
    "    return ds_all\n",
    "\n",
    "\n",
    "def grid_polygons_from_bounds(ds: xr.Dataset):\n",
    "    if 'x_bnds' not in ds or 'y_bnds' not in ds:\n",
    "        raise ValueError(\"notfound x_bnds / y_bnds；ensure nc have projection_*_bnds\")\n",
    "    xb = ds['x_bnds']; yb = ds['y_bnds']\n",
    "    if 'time' in xb.dims: xb = xb.isel(time=0)\n",
    "    if 'time' in yb.dims: yb = yb.isel(time=0)\n",
    "\n",
    "    x_b = np.asarray(xb)  # (Nx,2)\n",
    "    y_b = np.asarray(yb)  # (Ny,2)\n",
    "    Nx = x_b.shape[0]; Ny = y_b.shape[0]\n",
    "\n",
    "    rows, cols, geoms = [], [], []\n",
    "    for i in range(Ny):\n",
    "        y0, y1 = float(y_b[i,0]), float(y_b[i,1]); ys, yn = (y0, y1) if y0 <= y1 else (y1, y0)\n",
    "        for j in range(Nx):\n",
    "            x0, x1 = float(x_b[j,0]), float(x_b[j,1]); xw, xe = (x0, x1) if x0 <= x1 else (x1, x0)\n",
    "            geoms.append(box(xw, ys, xe, yn))\n",
    "            rows.append(i); cols.append(j)\n",
    "    grid_gdf = gpd.GeoDataFrame({'row': rows, 'col': cols}, geometry=geoms, crs=\"EPSG:27700\")\n",
    "    return grid_gdf, (Ny, Nx)\n",
    "\n",
    "\n",
    "def select_cells_with_internal_line(grid_gdf: gpd.GeoDataFrame, line_shp: str):\n",
    "    lines = gpd.read_file(line_shp).to_crs(27700)\n",
    "    if len(lines) == 0:\n",
    "        raise ValueError(f\"{os.path.basename(line_shp)} no items\")\n",
    "    sindex = lines.sindex\n",
    "\n",
    "    keep = []\n",
    "    for idx, cell in grid_gdf.iterrows():\n",
    "        cand = list(sindex.intersection(cell.geometry.bounds))\n",
    "        if not cand:\n",
    "            continue\n",
    "        inter = lines.iloc[cand].intersection(cell.geometry)\n",
    "        total_len = 0.0\n",
    "        for geom in inter:\n",
    "            if not geom.is_empty:\n",
    "                total_len += getattr(geom, \"length\", 0.0)\n",
    "        if total_len > 0:\n",
    "            keep.append(idx)\n",
    "\n",
    "    sel = grid_gdf.iloc[keep].copy()\n",
    "    mask_idx = pd.MultiIndex.from_arrays([sel['row'].values, sel['col'].values], names=['row','col'])\n",
    "    return sel, mask_idx\n",
    "\n",
    "\n",
    "def make_selected_mask(ds: xr.Dataset, mask_idx: pd.MultiIndex):\n",
    "    Ny, Nx = ds.sizes['y'], ds.sizes['x']\n",
    "    rows = np.asarray(mask_idx.get_level_values('row'))\n",
    "    cols = np.asarray(mask_idx.get_level_values('col'))\n",
    "    mask_np = np.zeros((Ny, Nx), dtype=bool)\n",
    "    mask_np[rows, cols] = True\n",
    "    mask = xr.DataArray(mask_np, coords={'y': ds['y'], 'x': ds['x']}, dims=('y','x'))\n",
    "    return mask, rows, cols\n",
    "\n",
    "\n",
    "def daily_mean_over_selected_cells(ds: xr.Dataset, mask_idx: pd.MultiIndex):\n",
    "    mask, _, _ = make_selected_mask(ds, mask_idx)\n",
    "    temp = ds['temp'].isel(member=0)  # (time,y,x)\n",
    "    return temp.where(mask).mean(dim=('y','x'), skipna=True)\n",
    "\n",
    "\n",
    "def pick_extreme_days(daily_mean: xr.DataArray):\n",
    "    imax = int(daily_mean.argmax('day').values)\n",
    "    imin = int(daily_mean.argmin('day').values)\n",
    "    vmax = float(daily_mean.isel(day=imax).values)\n",
    "    vmin = float(daily_mean.isel(day=imin).values)\n",
    "    med_val = float(np.median(daily_mean.values))\n",
    "    diff = np.abs(daily_mean.values - med_val)\n",
    "    imed = int(np.argmin(diff))\n",
    "    vmed = float(daily_mean.isel(day=imed).values)\n",
    "    return {'max': {'day': imax, 'value': vmax},\n",
    "            'min': {'day': imin, 'value': vmin},\n",
    "            'med': {'day': imed, 'value': vmed}}\n",
    "\n",
    "\n",
    "def build_average_year(ds: xr.Dataset) -> xr.Dataset:\n",
    "   \n",
    "    temp = ds['temp'].isel(member=0)\n",
    "    nt = temp.sizes['time']\n",
    "    if nt % 360 != 0:\n",
    "        raise ValueError(f\"time 1 {nt} not 360 times\")\n",
    "\n",
    "    day_coord = xr.DataArray(np.arange(nt) % 360, dims='time', name='day')\n",
    "    year_coord = xr.DataArray(np.arange(nt) // 360, dims='time', name='year')\n",
    "    temp = temp.assign_coords(day=day_coord, year=year_coord)\n",
    "\n",
    "    temp_avg = temp.groupby('day').mean(dim='time', skipna=True)\n",
    "\n",
    "    out = xr.Dataset({'temp_avg': temp_avg})\n",
    "    for c in ['x','y','lat','lon']:\n",
    "        if c in ds.coords:\n",
    "            out = out.assign_coords({c: ds[c]})\n",
    "    return out\n",
    "\n",
    "\n",
    "def summer_slice_from_average_year(ds_avg: xr.Dataset):\n",
    "    \"\"\"360 days，5–8 is day ∈ [120, 240) \"\"\"\n",
    "    all_days = np.arange(360)\n",
    "    summer_idx = all_days[(all_days >= 120) & (all_days < 240)]\n",
    "    return summer_idx\n",
    "\n",
    "\n",
    "def det_pf_on_cells(temp2d: xr.DataArray, mask_idx: pd.MultiIndex, mu: float, sigma: float) -> pd.DataFrame:\n",
    "\n",
    "    ds_tmp = temp2d.to_dataset(name='temp')\n",
    "    mask, _, _ = make_selected_mask(ds_tmp, mask_idx)\n",
    "    temp_cells = temp2d.where(mask).stack(cell=('y','x')).dropna('cell')\n",
    "\n",
    "\n",
    "    T = temp_cells.values.astype(np.float64)  # [n_cells]\n",
    "    pf = 0.5 * (1.0 + erf((T - mu) / (sigma * np.sqrt(2.0))))\n",
    "\n",
    "\n",
    "    Ny, Nx = mask.shape\n",
    "    flat_mask = mask.values.reshape(-1)\n",
    "    flat_rows = np.repeat(np.arange(Ny), Nx)\n",
    "    flat_cols = np.tile(np.arange(Nx), Ny)\n",
    "    sel_flat = flat_mask.nonzero()[0]\n",
    "    rows = flat_rows[sel_flat]\n",
    "    cols = flat_cols[sel_flat]\n",
    "\n",
    "    return pd.DataFrame({'row': rows, 'col': cols, 'pf': pf})\n",
    "\n",
    "\n",
    "def det_pf_cellwise_mu(temp2d: xr.DataArray, mask_idx: pd.MultiIndex, enhanced_idx_set,\n",
    "                       mu_enhanced: float, mu_default: float, sigma: float) -> pd.DataFrame:\n",
    "  \n",
    "    ds_tmp = temp2d.to_dataset(name='temp')\n",
    "    mask, _, _ = make_selected_mask(ds_tmp, mask_idx)\n",
    "    temp_cells = temp2d.where(mask).stack(cell=('y','x')).dropna('cell')\n",
    "\n",
    "    T = temp_cells.values.astype(np.float64)\n",
    "\n",
    "    Ny, Nx = mask.shape\n",
    "    flat_mask = mask.values.reshape(-1)\n",
    "    flat_rows = np.repeat(np.arange(Ny), Nx)\n",
    "    flat_cols = np.tile(np.arange(Nx), Ny)\n",
    "    sel_flat = flat_mask.nonzero()[0]\n",
    "    rows = flat_rows[sel_flat]\n",
    "    cols = flat_cols[sel_flat]\n",
    "\n",
    "    enh_set = set(enhanced_idx_set) if isinstance(enhanced_idx_set, (set, list, tuple)) else set()\n",
    "    is_enh = np.fromiter(((r, c) in enh_set for r, c in zip(rows, cols)), dtype=bool, count=T.size)\n",
    "    mu_vec = np.where(is_enh, mu_enhanced, mu_default).astype(np.float64)\n",
    "\n",
    "    pf = 0.5 * (1.0 + erf((T - mu_vec) / (sigma * np.sqrt(2.0))))\n",
    "    return pd.DataFrame({'row': rows, 'col': cols, 'pf': pf})\n",
    "\n",
    "\n",
    "\n",
    "def compute_length_for_cells(cells_gdf: gpd.GeoDataFrame, line_shp: str) -> pd.DataFrame:\n",
    "    lines = gpd.read_file(line_shp).to_crs(27700)\n",
    "    if len(lines) == 0:\n",
    "        raise ValueError(f\"{os.path.basename(line_shp)} no tiems\")\n",
    "    sindex = lines.sindex\n",
    "\n",
    "    out = []\n",
    "    for _, cell in cells_gdf.iterrows():\n",
    "        cand = list(sindex.intersection(cell.geometry.bounds))\n",
    "        total_len = 0.0\n",
    "        if cand:\n",
    "            inter = lines.iloc[cand].intersection(cell.geometry)\n",
    "            for geom in inter:\n",
    "                if not geom.is_empty:\n",
    "                    total_len += getattr(geom, \"length\", 0.0)\n",
    "        out.append({'row': int(cell['row']), 'col': int(cell['col']), 'length_m': float(total_len)})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def save_pf_maps_and_thr(rail_cells_gdf, ole_cells_gdf, pf_dict_rail, pf_dict_ole,\n",
    "                         thr, rail_shp, ole_shp, save_dir, save_prefix):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    try:\n",
    "        rail_lines = gpd.read_file(rail_shp).to_crs(27700)\n",
    "    except Exception:\n",
    "        rail_lines = None\n",
    "    try:\n",
    "        ole_lines = gpd.read_file(ole_shp).to_crs(27700)\n",
    "    except Exception:\n",
    "        ole_lines = None\n",
    "\n",
    "    def _save_one(asset_cells_gdf, df_pf, title, cmap, out_png, line_gdf=None):\n",
    "        plot_gdf = asset_cells_gdf.merge(df_pf[['row','col','pf']], on=['row','col'], how='left')\n",
    "        fig, ax = plt.subplots(figsize=(8,10))\n",
    "        plot_gdf.plot(column='pf', cmap=cmap, vmin=0, vmax=0.2, legend=True,\n",
    "                      ax=ax, edgecolor='grey', linewidth=0.2,\n",
    "                      missing_kwds={\"color\":\"lightgrey\"})\n",
    "        if line_gdf is not None and len(line_gdf) > 0:\n",
    "            line_gdf.plot(ax=ax, color='black', linewidth=0.5, alpha=0.7)\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.axis('off'); plt.tight_layout()\n",
    "        fig.savefig(out_png, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    for tag in ['max','med','min']:\n",
    "        if tag in pf_dict_rail and pf_dict_rail[tag] is not None:\n",
    "            _save_one(rail_cells_gdf, pf_dict_rail[tag], f\"RAIL PF — {tag} day\",\n",
    "                      \"Reds\", os.path.join(save_dir, f\"{save_prefix}_rail_pf_{tag}.png\"), rail_lines)\n",
    "        if tag in pf_dict_ole and pf_dict_ole[tag] is not None:\n",
    "            _save_one(ole_cells_gdf, pf_dict_ole[tag], f\"OLE PF — {tag} day\",\n",
    "                      \"Blues\", os.path.join(save_dir, f\"{save_prefix}_ole_pf_{tag}.png\"), ole_lines)\n",
    "\n",
    "\n",
    "    def _thr_plot(cells_gdf, df_pf_max, line_gdf, title, color, out_name):\n",
    "        idx = df_pf_max['pf'] > thr\n",
    "        thr_df = df_pf_max.loc[idx, ['row','col']]\n",
    "        thr_gdf = cells_gdf.merge(thr_df, on=['row','col'], how='inner')\n",
    "        fig, ax = plt.subplots(figsize=(8,10))\n",
    "        cells_gdf.plot(ax=ax, color='lightgrey', edgecolor='none')\n",
    "        thr_gdf.plot(ax=ax, color=color, edgecolor='black', linewidth=0.2)\n",
    "        if line_gdf is not None and len(line_gdf) > 0:\n",
    "            line_gdf.plot(ax=ax, color='black', linewidth=0.5, alpha=0.7)\n",
    "        ax.set_title(title, fontsize=12)\n",
    "        ax.axis('off'); plt.tight_layout()\n",
    "        fig.savefig(os.path.join(save_dir, out_name), dpi=300)\n",
    "        plt.close(fig)\n",
    "        return thr_gdf\n",
    "\n",
    "    rail_thr_gdf = _thr_plot(\n",
    "        rail_cells_gdf, pf_dict_rail['max'], rail_lines,\n",
    "        f\"RAIL PF > {thr} (max day)\", \"red\",\n",
    "        f\"{save_prefix}_rail_thr_maxday.png\"\n",
    "    )\n",
    "    ole_thr_gdf  = _thr_plot(\n",
    "        ole_cells_gdf, pf_dict_ole['max'], ole_lines,\n",
    "        f\"OLE PF > {thr} (max day)\",  \"blue\",\n",
    "        f\"{save_prefix}_ole_thr_maxday.png\"\n",
    "    )\n",
    "    return rail_thr_gdf, ole_thr_gdf\n",
    "\n",
    "\n",
    "def sum_length_and_material(thr_cells_gdf: gpd.GeoDataFrame, line_shp: str, ton_per_km: float, label: str):\n",
    "    if len(thr_cells_gdf) == 0:\n",
    "        detail = pd.DataFrame(columns=['row','col','length_m','length_km','material_t'])\n",
    "        summary = {'label': label, 'cells': 0, 'length_m': 0.0, 'length_km': 0.0, 'material_t': 0.0}\n",
    "        return summary, detail\n",
    "    len_df = compute_length_for_cells(thr_cells_gdf, line_shp)\n",
    "    len_df['length_km'] = len_df['length_m'] / 1000.0\n",
    "    len_df['material_t'] = len_df['length_km'] * ton_per_km\n",
    "    summary = {\n",
    "        'label': label,\n",
    "        'cells': int(len_df.shape[0]),\n",
    "        'length_m': float(len_df['length_m'].sum()),\n",
    "        'length_km': float(len_df['length_km'].sum()),\n",
    "        'material_t': float(len_df['material_t'].sum())\n",
    "    }\n",
    "    return summary, len_df[['row','col','length_m','length_km','material_t']]\n",
    "\n",
    "\n",
    "def _length_km_from_thr(thr_gdf: gpd.GeoDataFrame, line_shp: str) -> float:\n",
    "    if len(thr_gdf) == 0:\n",
    "        return 0.0\n",
    "    len_df = compute_length_for_cells(thr_gdf, line_shp)\n",
    "    return float(len_df['length_m'].sum() / 1000.0)\n",
    "\n",
    "\n",
    "\n",
    "def process_one_period(ds_period: xr.Dataset,\n",
    "                        rail_cells_gdf: gpd.GeoDataFrame,\n",
    "                        ole_cells_gdf: gpd.GeoDataFrame,\n",
    "                        rail_mask_idx: pd.MultiIndex,\n",
    "                        ole_mask_idx: pd.MultiIndex,\n",
    "                        period_tag: str,\n",
    "                        save_nc_path: str):\n",
    " \n",
    "\n",
    "    ds_avg = build_average_year(ds_period)  # {'temp_avg': (day,y,x)}\n",
    "\n",
    "\n",
    "    summer_idx = summer_slice_from_average_year(ds_avg)\n",
    "    daily_mean = ds_avg['temp_avg'].where(make_selected_mask(ds_period, rail_mask_idx)[0]).mean(dim=('y','x'), skipna=True)\n",
    "    daily_mean_summer = daily_mean.isel(day=summer_idx)\n",
    "    picks_rel = pick_extreme_days(daily_mean_summer)\n",
    "    picks = {tag: {'day': int(summer_idx[info['day']]), 'value': info['value']} for tag, info in picks_rel.items()}\n",
    "\n",
    "\n",
    "    sel_days = [picks['max']['day'], picks['med']['day'], picks['min']['day']]\n",
    "    temp_sel = ds_avg['temp_avg'].isel(day=sel_days)\n",
    "    ds_save = xr.Dataset({'temp': temp_sel.rename({'day':'sel_day'})})\n",
    "    ds_save = ds_save.assign_coords(sel_day=('sel_day', sel_days))\n",
    "    ds_save.to_netcdf(save_nc_path)\n",
    "\n",
    "    \n",
    "    day_temps = {tag: ds_avg['temp_avg'].isel(day=info['day']) for tag, info in picks.items()}\n",
    "    pf_rail = {}\n",
    "    pf_ole  = {}\n",
    "    for tag, t2d in day_temps.items():\n",
    "        pf_rail[tag] = det_pf_on_cells(t2d, rail_mask_idx, mu=RAIL_MU0, sigma=RAIL_SIG)\n",
    "        pf_ole[tag]  = det_pf_on_cells(t2d, ole_mask_idx,  mu=OLE_MU0,  sigma=OLE_SIG)\n",
    "\n",
    "\n",
    "    period_dir = os.path.join(OUT_DIR, period_tag)\n",
    "    os.makedirs(period_dir, exist_ok=True)\n",
    "    rail_thr_gdf, ole_thr_gdf = save_pf_maps_and_thr(\n",
    "        rail_cells_gdf=rail_cells_gdf,\n",
    "        ole_cells_gdf=ole_cells_gdf,\n",
    "        pf_dict_rail=pf_rail, pf_dict_ole=pf_ole, thr=THR,\n",
    "        rail_shp=RAIL_SHP, ole_shp=OLE_SHP,\n",
    "        save_dir=period_dir, save_prefix=f\"{period_tag}\"\n",
    "    )\n",
    "\n",
    "    return picks, day_temps, pf_rail, pf_ole, rail_thr_gdf, ole_thr_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec489c9-d10c-4b20-927f-66428da40f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished time=3600；future time=3600，space=(112,82)\n",
      "rail grid：1034；ole grid：451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21804\\2551200832.py:300: UserWarning: The GeoDataFrame you are attempting to plot is empty. Nothing has been displayed.\n",
      "  thr_gdf.plot(ax=ax, color=color, edgecolor='black', linewidth=0.2)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21804\\2551200832.py:300: UserWarning: The GeoDataFrame you are attempting to plot is empty. Nothing has been displayed.\n",
      "  thr_gdf.plot(ax=ax, color=color, edgecolor='black', linewidth=0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day(summer)：\n",
      "  current -> current: max=227, med=178, min=128 (day 索引 0-359)\n",
      "  future -> future: max=239, med=179, min=134 (day 索引 0-359)\n",
      "material saved：outputs_final_with_shapes5\\materials_future_maxday.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21804\\2551200832.py:300: UserWarning: The GeoDataFrame you are attempting to plot is empty. Nothing has been displayed.\n",
      "  thr_gdf.plot(ax=ax, color=color, edgecolor='black', linewidth=0.2)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21804\\2551200832.py:300: UserWarning: The GeoDataFrame you are attempting to plot is empty. Nothing has been displayed.\n",
      "  thr_gdf.plot(ax=ax, color=color, edgecolor='black', linewidth=0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced compare saved：outputs_final_with_shapes5\\compare_baseline_vs_future_post_maxday.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    ds_curr = open_period_nc(CLIMATE_DIR, CURR_KEY)\n",
    "    ds_fut  = open_period_nc(CLIMATE_DIR, FUT_KEY)\n",
    "    print(f\"finished time={ds_curr.sizes['time']}；future time={ds_fut.sizes['time']}，space=({ds_curr.sizes['y']},{ds_curr.sizes['x']})\")\n",
    "\n",
    "\n",
    "    grid_gdf, _ = grid_polygons_from_bounds(ds_curr)\n",
    "\n",
    "\n",
    "    rail_cells_gdf, rail_mask_idx = select_cells_with_internal_line(grid_gdf, RAIL_SHP)\n",
    "    ole_cells_gdf,  ole_mask_idx  = select_cells_with_internal_line(grid_gdf, OLE_SHP)\n",
    "    print(f\"rail grid：{len(rail_cells_gdf)}；ole grid：{len(ole_cells_gdf)}\")\n",
    "\n",
    "\n",
    "    curr_nc_path = os.path.join(OUT_DIR, \"current_three_days.nc\")\n",
    "    fut_nc_path  = os.path.join(OUT_DIR, \"future_three_days.nc\")\n",
    "\n",
    "    curr_picks, curr_day_temps, pf_curr_rail, pf_curr_ole, curr_thr_rail_gdf, curr_thr_ole_gdf = \\\n",
    "        process_one_period(ds_curr,\n",
    "                           rail_cells_gdf, ole_cells_gdf,\n",
    "                           rail_mask_idx, ole_mask_idx,\n",
    "                           period_tag=\"current\",\n",
    "                           save_nc_path=curr_nc_path)\n",
    "\n",
    "    fut_picks, fut_day_temps, pf_fut_rail, pf_fut_ole, fut_thr_rail_gdf, fut_thr_ole_gdf = \\\n",
    "        process_one_period(ds_fut,\n",
    "                           rail_cells_gdf, ole_cells_gdf,\n",
    "                           rail_mask_idx, ole_mask_idx,\n",
    "                           period_tag=\"future_pre\",\n",
    "                           save_nc_path=fut_nc_path)\n",
    "\n",
    "\n",
    "    def _picks_to_str(tag, picks):\n",
    "        return f\"{tag}: max={picks['max']['day']}, med={picks['med']['day']}, min={picks['min']['day']} (day 索引 0-359)\"\n",
    "    print(\"day(summer)：\")\n",
    "    print(\"  current ->\", _picks_to_str(\"current\", curr_picks))\n",
    "    print(\"  future ->\", _picks_to_str(\"future\", fut_picks))\n",
    "\n",
    "\n",
    "    fut_len_rail_sum, fut_rail_detail = sum_length_and_material(fut_thr_rail_gdf, RAIL_SHP, RAIL_TON_PER_KM, \"RAIL (future, max day)\")\n",
    "    fut_len_ole_sum,  fut_ole_detail  = sum_length_and_material(fut_thr_ole_gdf,  OLE_SHP,  OLE_TON_PER_KM,  \"OLE (future, max day)\")\n",
    "\n",
    "    mat_df = pd.DataFrame([fut_len_rail_sum, fut_len_ole_sum])\n",
    "    mat_csv = os.path.join(OUT_DIR, \"materials_future_maxday.csv\")\n",
    "    mat_df.to_csv(mat_csv, index=False)\n",
    "    if len(fut_rail_detail) > 0:\n",
    "        fut_rail_detail.to_csv(os.path.join(OUT_DIR, \"materials_future_maxday_rail_detail.csv\"), index=False)\n",
    "    if len(fut_ole_detail) > 0:\n",
    "        fut_ole_detail.to_csv(os.path.join(OUT_DIR, \"materials_future_maxday_ole_detail.csv\"), index=False)\n",
    "    print(f\"material saved：{mat_csv}\")\n",
    "\n",
    "\n",
    "    rail_enhanced_set = set(zip(fut_thr_rail_gdf['row'].tolist(), fut_thr_rail_gdf['col'].tolist()))\n",
    "    ole_enhanced_set  = set(zip(fut_thr_ole_gdf['row'].tolist(),  fut_thr_ole_gdf['col'].tolist()))\n",
    "\n",
    "    pf_fut_post_rail = {}\n",
    "    pf_fut_post_ole  = {}\n",
    "    for tag in ['max','med','min']:\n",
    "        pf_fut_post_rail[tag] = det_pf_cellwise_mu(\n",
    "            fut_day_temps[tag], rail_mask_idx,\n",
    "            enhanced_idx_set=rail_enhanced_set,\n",
    "            mu_enhanced=RAIL_MU1, mu_default=RAIL_MU0,\n",
    "            sigma=RAIL_SIG\n",
    "        )\n",
    "        pf_fut_post_ole[tag] = det_pf_cellwise_mu(\n",
    "            fut_day_temps[tag], ole_mask_idx,\n",
    "            enhanced_idx_set=ole_enhanced_set,\n",
    "            mu_enhanced=OLE_MU1, mu_default=OLE_MU0,\n",
    "            sigma=OLE_SIG\n",
    "        )\n",
    "\n",
    "    fut_post_dir = os.path.join(OUT_DIR, \"future_post\")\n",
    "    os.makedirs(fut_post_dir, exist_ok=True)\n",
    "    rail_thr_post_gdf, ole_thr_post_gdf = save_pf_maps_and_thr(\n",
    "        rail_cells_gdf=rail_cells_gdf,\n",
    "        ole_cells_gdf=ole_cells_gdf,\n",
    "        pf_dict_rail=pf_fut_post_rail, pf_dict_ole=pf_fut_post_ole, thr=THR,\n",
    "        rail_shp=RAIL_SHP, ole_shp=OLE_SHP,\n",
    "        save_dir=fut_post_dir, save_prefix=\"future_post\"\n",
    "    )\n",
    "\n",
    "\n",
    "    rail_len_curr_km  = _length_km_from_thr(curr_thr_rail_gdf, RAIL_SHP)\n",
    "    rail_len_post_km  = _length_km_from_thr(rail_thr_post_gdf, RAIL_SHP)\n",
    "    ole_len_curr_km   = _length_km_from_thr(curr_thr_ole_gdf,  OLE_SHP)\n",
    "    ole_len_post_km   = _length_km_from_thr(ole_thr_post_gdf,  OLE_SHP)\n",
    "\n",
    "    def _compare(pre_km, post_km):\n",
    "        if pre_km <= 0:\n",
    "            return 0.0, 0.0\n",
    "        delta = pre_km - post_km\n",
    "        pct = 100.0 * delta / pre_km\n",
    "        return delta, pct\n",
    "\n",
    "    rail_delta_km, rail_reduct_pct = _compare(rail_len_curr_km, rail_len_post_km)\n",
    "    ole_delta_km,  ole_reduct_pct  = _compare(ole_len_curr_km,  ole_len_post_km)\n",
    "\n",
    "    cmp_df = pd.DataFrame([\n",
    "        {'asset':'RAIL','baseline_km':rail_len_curr_km,'post_km':rail_len_post_km,'delta_km':rail_delta_km,'reduction_%':rail_reduct_pct},\n",
    "        {'asset':'OLE', 'baseline_km':ole_len_curr_km, 'post_km':ole_len_post_km, 'delta_km':ole_delta_km, 'reduction_%':ole_reduct_pct}\n",
    "    ])\n",
    "    cmp_csv = os.path.join(OUT_DIR, \"compare_baseline_vs_future_post_maxday.csv\")\n",
    "    cmp_df.to_csv(cmp_csv, index=False)\n",
    "    print(f\"enhanced compare saved：{cmp_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
